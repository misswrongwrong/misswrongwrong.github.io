<h2 id="1-introduction-to-reinforcement-learning">1: Introduction to Reinforcement Learning</h2>
<p>课程网站<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html">课件+视频</a></p>

<h2 id="1-机器学习的分支">1. 机器学习的分支：</h2>
<p>1.监督式学习（有标签），2.非监督式学习（无标签），3.增强学习（无标签，有环境反馈) <br />
增强学习和其他方法不同之处在于：</p>

<ul>
  <li>不像监督式学习有标签，它只有奖励信号(reward signal)</li>
  <li>反馈是延迟的，而不是立刻的</li>
  <li>时间很重要</li>
  <li>agent（可以理解为机器人，或者在图2中的人脑）的行为影响接下来一系列接受到的反馈数据
<img src="/assets/images/L1-1.png" alt=" " title="machine learning branches" height="260px" width="350px" /></li>
</ul>

<h2 id="2-增强学习基本概念">2. 增强学习基本概念</h2>
<p>典型的增强学习场景：Agent（人脑）控制动作，这些动作作用于环境（地球），环境给予反馈。人脑通过观察环境，和分析奖惩，再做下一个决定。<em>这在日常生活中很常见，也很容易理解。比如你这一秒在写作业，下一秒决定去玩（动作），结果会被爸妈打（反馈），所以被迫回来继续写作业（下一个动作）。当然了，你不会一下子就那么听话的，经过多次反馈，你就学会了对于自己有益的策略。</em></p>

<ul>
  <li>在时刻t的时候, agent人脑：
    <ul>
      <li>执行动作 $ A_t $</li>
      <li>接收到环境的观测值$O_t$</li>
      <li>接收到环境的奖励$R_t$</li>
    </ul>
  </li>
  <li>环境：
    <ul>
      <li>接收到行为$A_t$</li>
      <li>释放观测信号$O_{t+1}$</li>
      <li>释放奖励信号$R_{t+1}$</li>
    </ul>
  </li>
  <li>t 时间增加
<img src="/assets/images/L1-2.png" alt=" " title="RL basic concetps" /></li>
</ul>

<h2 id="3-对于变量的解释">3. 对于变量的解释</h2>
<p>随着时间的增长，你会有很多关于观测$O_t$，行为$A_t$和奖励$R_t$的序列，我们可以称它为历史$H_t$。
<script type="math/tex">H_t = O_1, R_1, A_1, ... , A_{t-1}, O_{t-1},R_{t-1}</script>
在增强学习里，你常会遇见一个$S_t$状态变量，这个状态变量就是对于历史的综合处理$S_t = f(H_t)$， 它可以用来对未来发生的事情做判断和决定。我们不是常说从历史中吸取经验教训嘛，就是这个道理。</p>

<p><em>其实我很佩服Rich Sutton，他提出的增强学习模型很简单，但是很贴近人脑的运作方式。比如说从历史中学习经验教训，还有explore和exploit的行为模式（人在有些时候会探索未知、尝试新鲜的事情，另外一些时候我们会采取对于自己最优的策略。）从某种意义上，我觉得他的算法是很哲学的思考的结果。</em></p>

<p>$S_t$具体分有环境的状态$S_t^e$和Agent的状态$S_t^a$，当环境是完全可观测的（fully observable)，也就是Agent能直接看到环境的状态的时候，$S_t^e$和$S_t^a$就是等同的。然而，如果环境是部分可观测的(partially observable)，Agent获得到的就不是直接的环境状态。</p>

<h2 id="4-增强学习agent的组成">4. 增强学习Agent的组成</h2>
<p>一个增强学习的Agent可能包括一个或者更多个以下的组成部分：</p>
<ul>
  <li><strong>策略（Policy)</strong>: Agent如何做行为决策</li>
  <li><strong>价值函数(Value Function)</strong>: 判断一个状态 与/或 一个行为的好坏</li>
  <li><strong>模型(Model)</strong>: Agent对于环境的表达</li>
</ul>

<p><strong>策略（Policy)</strong></p>
<ul>
  <li>它是Agent的行为方式。</li>
  <li>给定一个状态S，经过策略，可以得到一个行为A。所以策略是从状态到行为的映射。</li>
  <li>决定策略 (Determitistic Policy),在状态S下，Agent一定会执行动作a。<br />
$a = \pi(s)$</li>
  <li>随机策略 (Stochastic Policy)， 在状态s下，Agent执行a的概率。<br />
$\pi(a|s)=p[A_t = a| S_t = s] $</li>
</ul>

<p><strong>价值函数(Value Function)</strong></p>
<ul>
  <li>它是对于未来奖惩的预测</li>
  <li>被用于评估一个状态的好坏</li>
  <li>因此被用于选择行为A，例如以下是对于未来获得奖励的一个期望：<br />
$V_{\pi}(s) = E_{\pi}[ R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+…. | S_t = s]$</li>
</ul>

<p><strong>模型(Model)</strong>:</p>
<ul>
  <li>它是用于预测接下来环境会做什么</li>
  <li>P 预测下一个状态</li>
  <li>R 预测下一个即刻奖惩 (reward),例如：
$ P_{ss’}^a = P[S_{t+1}= s’ | S_t = s , A_t = a ]$<br />
$ R_s^a = E [R_{t+1}| S_t = s, A_t = a ]$</li>
</ul>

<p>接下来课件里举了一个迷宫的例子(Maze example)，帮助你理解之前介绍的所有概念，可以跟着课件细细看，这里就不细讲了。</p>

<h2 id="5-增强学习的分类">5. 增强学习的分类</h2>
<ul>
  <li>基于价值(value based)
    <ul>
      <li>价值函数</li>
    </ul>
  </li>
  <li>基于策略(policy based)
    <ul>
      <li>策略方法</li>
    </ul>
  </li>
  <li>演员评判家 (Actor Critic )
    <ul>
      <li>既有价值函数，又有策略方法</li>
    </ul>
  </li>
</ul>

<p>另一种是根据模型来分类：Model Free是指不需要去猜测environment的工作方式，而Model based则是需要学习environment的工作方式。</p>

<p><img src="/assets/images/L1-3.png" alt=" " title="RL categorizing" /></p>

<h2 id="6-增强学习和规划planning的异同">6. 增强学习和规划(planning)的异同</h2>
<ul>
  <li>增强学习
    <ul>
      <li>环境是部分已知</li>
      <li>Agent和环境交互</li>
      <li>Agent改进自己的策略</li>
    </ul>
  </li>
  <li>规划
    <ul>
      <li>环境模型已知</li>
      <li>Agent根据模型进行计算（不交互）</li>
      <li>Agent 改进自己的策略</li>
    </ul>
  </li>
</ul>

<h2 id="7-探索和利用exploration-and-exploitation">7. 探索和利用(Exploration and Exploitation)</h2>
<p>总而言之，增强学习是一种试错学习（trial-and-error), Agent能够通过和环境交互中的经验，发现一个好的策略。增强学习选择动作，包括探索和利用。探索是为了发现更多关于环境的信息，利用是为了充分利用已有的经验而最大化奖励。</p>

<p>#<a href="http://localhost:4000/2017/11/RL_notes/">更多RL文章</a></p>
