<h2 id="第二节-supervised-learning-and-imitation">第二节: Supervised learning and imitation</h2>
<p>*Berkeley课程网站<a href="http://rll.berkeley.edu/deeprlcourse/">课件+视频</a><br />
第二节内容：
| 内容      | 话题                                              |
| ——— | ————————————————-
| 定义      | 序列决定问题           |
| 介绍      | 模仿学习    |
| 前沿工作  | 模仿学习               |
| 缺点      | 模仿学习               |</p>

<p><strong>1. 什么是Deep RL？</strong> <br />
答: Deep RL包含了deep learing（深度学习）和reinforcement learning（增强学习）。</p>

<p><strong>2. 什么是Reinforcement learning？</strong> <br />
答：增强学习就通过和环境互动，从环境中获得奖励来学习的一种方式。很通俗的一个例子是我们训练狗的时候，让它做一个动作，做的好就给肉吃（奖励），做的不好就挨打（惩罚）。通过一段时间的学习，这只狗就知道如何获得奖励不挨打。这整个训练的过程，狗是无法与人语言沟通的，在最开始并不知道应该怎么做可以吃肉，它是完全经过自己的经历和体验获得的这种“智慧”。</p>

<p>再如下图，是经典的RL模型，机器人做决定，执行行为（actions），环境会对行为给出反馈。这个反馈包括了新的观测结果和奖励反馈。这个反馈反作用于机器人，让它作下一次的行为决定。这个机器人和环境间互相作用，反复循环的模型就是RL。</p>

<p><img src="/assets/images/DRL1-1.png" alt="Fig. 1  " title="RL" height="260px" width="350px" /></p>

<p><strong>3. Deep RL 现在可以做什么？</strong> <br />
 答： 1) 通过简单和已知的法则，在某些领域得到高水平的熟练程度 2）只用简单的输入设备，通过足够多的经验训练，学习简单的行为 3）学习人类提供的专家行为。</p>

<p><strong>4. 为什么要深度学习呢？</strong> <br />
答：<strong>因为深度模型能帮助增强学习算法“端对端”(end-to-end)地解决复杂的问题。</strong></p>

<p>要理解”端对端“，可以看下面两张图。图1中，上面一行是一个标准的机器视觉算法例子：输入图像，提取特征，提取中层特征，分类器输出分类结果。下面一行是一个深度学习“端对端”的例子：输入图像，通过深度神经网络，直接输出分类结果。当然，这个深度神经网络是经过大量的图像-类别训练过的。
<img src="/assets/images/DRL1-2.png" alt="Fig. 2  " title="end to end1" /></p>

<p>图2是在机器人领域的一个例子。上面一行是一个标准的机器人控制流程：观测数据，状态估计，模型预测，规划，低水平的控制，输出机器人控制信号。下面一行是深度学习“端对端”的例子：输入观测值，通过神经网络，直接输出机器人控制信号。和机器视觉那个例子一样，这个深度网络也是需要通过大量的观测值-机器人控制信号的训练样本训练。
<img src="/assets/images/DRL1-3.png" alt="Fig. 3 " title="end to end2" /></p>

<p><strong>5. 什么时候要用增强学习呢？</strong> <br />
答：序列决策的时候。什么是序列决策呢？当你的系统是单一独立的决定，不影响到未来的决定的时候，就不需要序列决策，比如分类、逻辑回归。相反，当你的当前决定影响到未来决定的时候，你就需要考虑序列决策，比如机器人控制、自动车驾驶、语言对话和金融模型。</p>

<p><strong>6. 为什么现在要学习Deep RL呢？</strong><br />
 答： 因为深度学习，增强学习以及计算能力的最新进展，创造了特别好的环境，Deep RL 是应运而生。</p>

<p><strong>7. 除了增强学习，还有其他形式的监督式学习吗?</strong><br />
 答： 1) 模仿学习(Learning from demonstrations): 直接从观测行为中推断奖励。2) 从观测世界中学习(Learning from observing the world): 非监督式的方式学习预测。3) 任务学习（Learning from other tasks): 迁移学习，学习如何学习。</p>

<p><strong>8. Deep RL 现在可以做什么？</strong> <br />
 答： 1) 通过简单和已知的法则，在某些领域得到高水平的熟练程度 2）只用简单的输入设备，通过足够多的经验训练，学习简单的行为 3）学习人类提供的专家行为。</p>

<p><strong>9. Deep RL 的挑战是什么？</strong> <br />
 答： 1) 人类学的很快，而Deep RL通常很慢 2）人类可以重新使用以前的只是，而迁移学习还是一个问题 3）奖励机制和预测模型应该如何设计，还不清楚。</p>

