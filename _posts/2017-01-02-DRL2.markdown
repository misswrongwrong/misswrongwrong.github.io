---
layout: post
title: Supervised learning and imitation [2]
date: 2017-01-02 20:45:00 +01:00
categories: tech
tags: DRL
---
## 第二节: Supervised learning and imitation
*Berkeley课程网站[课件+视频](http://rll.berkeley.edu/deeprlcourse/) 

* 本节内容：
	* 定义:        序列决定问题
	* 介绍:        模仿学习
	* 前沿工作举例:   模仿学习
	*  缺点:        模仿学习

**1. 强化学习(reinforcement learning）符号表示** \\
强化学习的结果是机器接收观测或状态，经过一定策略来产生行为的，如图1所示。其中，$$o_t$$ 表示t时刻的观测，$$s_t$$表示t时刻的状态，$$a_t$$表示t时刻机器人采取的动作, $$\pi_{\theta}(a_t|o_t)$$表示基于观测的动作策略。策略有决定策略(Determitistic Policy)和随机策略(Stochastic Policy)两种。决定策略：在观测值为$$o_t$$时，机器人一定执行动作a，$$a_t = \pi_{\theta}(o_t)$$。随机策略：在观测值$$o_t$$时，机器人执行a的概率是$$\pi_{\theta}(a_t|o_t) $$。

其中 $$o_t$$ 和 $$s_t$$ 不同为： $$o_t$$ 往往表示机器所感知到的信息（如由像素组成的图像），而 $$s_t$$ 表示外界环境实际所处的状态（如物体运动的速度，物体相对位置等）。它们不一定能观测到，是高维像素的低维内在表示，而且状态是满足马尔科夫性质的，但观测就不一定满足了。当环境是完全可观测的（fully observable)，$$s_t$$和$$o_t$$就是等同的。然而，如果环境是部分可观测的(partially observable)，机器人获得到的就不是直接的环境状态。

图1下方描述的是马尔可夫决策过程（Markov Decision Processes，MDP）, 即假设当前状态只与上一状态和动作有关。
$$p(s_{t+1} | s_t, a_t)$$表示状态转移函数，$$s_{t+1}$$仅和$$s_t$$与$$a_t$$有关。

![Fig. 1](/assets/images/DRL2-1.png  "DRL")
